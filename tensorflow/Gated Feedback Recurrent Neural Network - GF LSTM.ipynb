{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Feedback Recurrent Neural Network\n",
    "\n",
    "This notebook contains a Tensorflow (http://www.tensorflow.org) implementation of the Gated Feedback Recurrent Neural Network (the LSTM version) from this paper: http://arxiv.org/pdf/1502.02367v4.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "import numpy as np\n",
    "\n",
    "train_data, valid_data, test_data, vocab = reader.ptb_raw_data('simple-examples/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 20\n",
    "num_steps = 20\n",
    "hidden_size = 200\n",
    "emb_size = 200 # Note: this is kind of a cheat. This will *not* work if emb_size != hidden_size\n",
    "vocab_size = 10000\n",
    "epochs = 2\n",
    "init_scale = 0.1\n",
    "num_hidden_layers = 1\n",
    "\n",
    "lr = tf.placeholder(tf.float32, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Build Model\n",
    "session = tf.Session()\n",
    "\n",
    "X = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "targets = tf.placeholder(tf.int64, [batch_size, num_steps])\n",
    "\n",
    "embedding = tf.Variable(\n",
    "  tf.random_uniform([vocab_size, emb_size], minval=-init_scale, maxval=init_scale),\n",
    "  name=\"embedding\")\n",
    "\n",
    "# For input gate.\n",
    "Wi = [tf.Variable(\n",
    "  tf.random_uniform([emb_size, hidden_size], minval=-init_scale, maxval=init_scale),\n",
    "  name=\"Wi_%d\" % i) for i in range(num_hidden_layers)]\n",
    "Ui = [tf.Variable(\n",
    "  tf.random_uniform([hidden_size, hidden_size], minval=-init_scale, maxval=init_scale),\n",
    "  name=\"Ui_%d\" % i) for i in range(num_hidden_layers)]\n",
    "\n",
    "# For forget gate.\n",
    "Wf = [tf.Variable(\n",
    "  tf.random_uniform([emb_size, hidden_size], minval=-init_scale, maxval=init_scale),\n",
    "  name=\"Wf_%d\" % i) for i in range(num_hidden_layers)]\n",
    "Uf = [tf.Variable(\n",
    "  tf.random_uniform([hidden_size, hidden_size], minval=-init_scale, maxval=init_scale),\n",
    "  name=\"Uf_%d\" % i) for i in range(num_hidden_layers)]\n",
    "\n",
    "# For content -- Quick note: there's no transformation from content -> state. They are both\n",
    "# the same size.\n",
    "Wc = [tf.Variable(\n",
    "  tf.random_uniform([emb_size, hidden_size], minval=-init_scale, maxval=init_scale),\n",
    "  name=\"Wc_%d\" % i) for i in range(num_hidden_layers)]\n",
    "Uc = [tf.Variable(\n",
    "  tf.random_uniform([hidden_size, hidden_size], minval=-init_scale, maxval=init_scale),\n",
    "  name=\"Uc_%d\" % i) for i in range(num_hidden_layers)]\n",
    "\n",
    "# For hidden state output gate.\n",
    "Wo = [tf.Variable(\n",
    "  tf.random_uniform([emb_size, hidden_size], minval=-init_scale, maxval=init_scale),\n",
    "  name=\"Wo_%d\" % i) for i in range(num_hidden_layers)]\n",
    "Uo = [tf.Variable(\n",
    "  tf.random_uniform([hidden_size, hidden_size], minval=-init_scale, maxval=init_scale),\n",
    "  name=\"Uo_%d\" % i) for i in range(num_hidden_layers)]\n",
    "\n",
    "# For gated feedback gates (e.g. the contribution of the paper).\n",
    "Wg = [tf.Variable(\n",
    "  tf.random_uniform([emb_size, 1], minval=-init_scale, maxval=init_scale),\n",
    "  name=\"Wg_%d\" % i) for i in range(num_hidden_layers)]\n",
    "Ug = [tf.Variable(\n",
    "  tf.random_uniform([hidden_size * num_hidden_layers, 1], minval=-init_scale, maxval=init_scale),\n",
    "  name=\"Ug_%d\" % i) for i in range(num_hidden_layers)]\n",
    "\n",
    "# For output.\n",
    "output_weights = tf.Variable(\n",
    "  tf.random_uniform([hidden_size, vocab_size], minval=-init_scale, maxval=init_scale),\n",
    "  name=\"output_weights\")\n",
    "output_bias = tf.Variable(tf.zeros([vocab_size]), name=\"output_bias\")\n",
    "\n",
    "X_in = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "initial_state = tf.zeros([batch_size, hidden_size])\n",
    "content = initial_state\n",
    "state = [initial_state] * num_hidden_layers\n",
    "prev_concat_h = tf.zeros([batch_size, hidden_size * num_hidden_layers])\n",
    "loss = tf.zeros([])\n",
    "# TODO: prev concat h\n",
    "for time_step in range(num_steps):\n",
    "  h_prev = X_in[:, time_step, :]\n",
    "  for layer in range(num_hidden_layers):\n",
    "    input_gate = tf.nn.sigmoid(tf.matmul(h_prev, Wi[layer])  + tf.matmul(state[layer], Ui[layer]))\n",
    "    forget_gate = tf.nn.sigmoid(tf.matmul(h_prev, Wf[layer]) + tf.matmul(state[layer], Uf[layer]))\n",
    "    output_gate = tf.nn.sigmoid(tf.matmul(h_prev, Wo[layer]) + tf.matmul(state[layer], Uo[layer]))\n",
    "    \n",
    "    # Main contribution of paper:\n",
    "    gates = [tf.sigmoid(tf.matmul(h_prev, Wg[i]) + tf.matmul(prev_concat_h, Ug[i])) for i in range(num_hidden_layers)]\n",
    "    gated_prev_timestep = [gates[i] * tf.matmul(state[layer], Uc[i]) for i in range(num_hidden_layers)]\n",
    "    new_content = tf.nn.tanh(tf.matmul(h_prev, Wc[layer]) + tf.add_n(gated_prev_timestep))\n",
    "    \n",
    "    content = tf.mul(forget_gate, content) + tf.mul(input_gate, new_content)\n",
    "    state[layer] = tf.mul(output_gate, tf.nn.tanh(content))\n",
    "    \n",
    "  logits = tf.nn.bias_add(tf.matmul(state[num_hidden_layers-1], output_weights), output_bias)\n",
    "  step_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, targets[:, time_step])\n",
    "  loss += tf.reduce_sum(step_loss)\n",
    "  prev_concat_h = tf.concat(1, state)\n",
    "\n",
    "final_state = state\n",
    "cost = loss / batch_size\n",
    "\n",
    "tf.scalar_summary(\"cost\", cost)\n",
    "merged = tf.merge_all_summaries()\n",
    "writer = tf.train.SummaryWriter(\"summaries/gfrnn\", session.graph_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "1000 1536.86554276\n",
      "2000 1075.77593235\n",
      "3000 845.104395239\n",
      "4000 718.76732411\n",
      "5000 641.0083012\n",
      "6000 593.578288039\n",
      "7000 552.39057359\n",
      "8000 518.773597082\n",
      "9000 490.011530283\n",
      "10000 469.79929651\n",
      "11000 445.811142429\n",
      "12000 427.981149944\n",
      "13000 413.231142047\n",
      "14000 399.589796646\n",
      "15000 387.374589524\n",
      "16000 375.535316084\n",
      "17000 364.656415065\n",
      "18000 357.716949046\n",
      "19000 350.007875462\n",
      "20000 340.5189465\n",
      "21000 334.729726814\n",
      "22000 328.91687226\n",
      "23000 323.211649227\n",
      "24000 315.498545393\n",
      "25000 309.774060383\n",
      "26000 303.523051557\n",
      "27000 297.389077065\n",
      "28000 292.475912875\n",
      "29000 287.393987197\n",
      "30000 283.363092117\n",
      "31000 278.863735412\n",
      "32000 275.836836503\n",
      "33000 272.480957052\n",
      "34000 269.913912177\n",
      "35000 266.354806615\n",
      "36000 263.881540787\n",
      "37000 260.255694866\n",
      "38000 255.84195618\n",
      "39000 253.142906497\n",
      "40000 250.839347938\n",
      "41000 247.700747125\n",
      "42000 244.609508041\n",
      "43000 241.109089536\n",
      "44000 238.690587697\n",
      "45000 236.178669209\n",
      "46000 234.919347477\n",
      "Epoch 1\n",
      "47000 233.529050243\n",
      "48000 231.346767272\n",
      "49000 228.853817309\n",
      "50000 225.70174362\n",
      "51000 223.120431433\n",
      "52000 221.306655681\n",
      "53000 219.796232818\n",
      "54000 218.247005976\n",
      "55000 216.268173971\n",
      "56000 214.857019061\n",
      "57000 212.859998202\n",
      "58000 210.188553206\n",
      "59000 209.079580721\n",
      "60000 207.045767747\n",
      "61000 205.196654393\n",
      "62000 203.450599296\n",
      "63000 201.735644431\n",
      "64000 200.189708323\n",
      "65000 198.788259576\n",
      "66000 197.335555278\n",
      "67000 195.543393851\n",
      "68000 194.538999587\n",
      "69000 193.175939226\n",
      "70000 191.430713621\n",
      "71000 189.920810466\n",
      "72000 188.439249937\n",
      "73000 186.775430173\n",
      "74000 185.052030742\n",
      "75000 183.768886451\n",
      "76000 182.437888552\n",
      "77000 181.030238461\n",
      "78000 179.976427611\n",
      "79000 178.762464736\n",
      "80000 177.910351761\n",
      "81000 176.838355055\n",
      "82000 175.814496864\n",
      "83000 174.593081319\n",
      "84000 173.129086523\n",
      "85000 171.620987287\n",
      "86000 170.677194178\n",
      "87000 169.506480399\n",
      "88000 168.378289044\n",
      "89000 166.883673974\n",
      "90000 165.782215237\n",
      "91000 164.632383932\n",
      "92000 163.851626382\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "session.run(tf.initialize_all_variables())\n",
    "sgd = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "costs = 0.0\n",
    "iters = 0\n",
    "for i in range(epochs):\n",
    "  print 'Epoch', i\n",
    "  for step, (x, y) in enumerate(reader.ptb_iterator(train_data, batch_size, num_steps)):\n",
    "    result, step_cost, _, = session.run([merged, cost, sgd],\n",
    "                             {X: x, targets: y, lr: 1.0 / (i + 1)})\n",
    "    costs += step_cost\n",
    "    iters += num_steps\n",
    "    if iters % 1000 == 0:\n",
    "      print iters, np.exp(costs / iters)\n",
    "      writer.add_summary(result, iters)\n",
    "      writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
